{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link =\"https://en.wikipedia.org/wiki/20th_Century%27s_Greatest_Hits:_100_English-Language_Books_of_Fiction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very sad story is about to begin :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class website:\n",
    "    '''\n",
    "    Class for the wiki website \n",
    "    '''\n",
    "    def  __init__(self, link):\n",
    "        '''\n",
    "        Define e.g. where the module with links is \n",
    "        '''\n",
    "        self.link = link\n",
    "        r = requests.get(link)\n",
    "        r.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(r.text,'lxml')\n",
    "        self.books_module = self.findAll('li') # this is the module with links, welll... it should be. As a form of bs4\n",
    "\n",
    "        \n",
    "class book_page(website):\n",
    "    '''\n",
    "    Class for the particular book page = contains book author, name link to the book page etc. as an attribute\n",
    "    '''\n",
    "    def __init__(self, link):\n",
    "        \n",
    "        self.link = link\n",
    "        r = requests.get(link)\n",
    "        r.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(r.text,'lxml')\n",
    "        self.name = \"\"\n",
    "        self.author = \"\"\n",
    "        self.country = \"\"\n",
    "        \n",
    "        \n",
    "    def GetName(self):\n",
    "        '''\n",
    "         Scrape book name from book page method\n",
    "         '''   \n",
    "        self.name = self.soup.find('h1').text.strip()\n",
    "        \n",
    "        \n",
    "    def GetAuthor (self):\n",
    "        '''\n",
    "         Scrape book author from book page method\n",
    "        '''  \n",
    "        self.author  = self.soup.find('table',{'class':'infobox vcard'}).text\n",
    " #no time for country\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WIKI_downloader:\n",
    "\n",
    "    def __init__(self, link):\n",
    "        '''\n",
    "        What are we about to extract?\n",
    "        '''   \n",
    "        self.main_link = link #link of the main  page\n",
    "        self.main_website = website(link) #website object based on the lik\n",
    "        self.main_soup = self.main_website.soup #it's soup\n",
    "        self.book_hrefs = [] #list of url pieces scraped from the list of books on this main page \n",
    "        self.book_links = [] #actual URL links built on those href pieces\n",
    "        self.count_links = 0 #their number\n",
    "        self.book_pages = []#page objects on the book_liks\n",
    "        self.output = pd.DataFrame(columns = [\"Title\", \"Author\",\"Country\"]) #the pandastable\n",
    "        \n",
    "    def ListLinks(self):\n",
    "        '''\n",
    "        Extract the link endings for particular books\n",
    "        Make them actual links\n",
    "        '''\n",
    "        self.book_hrefs = self.main_website.books_module.findAll('a',{'class':'external text'})\n",
    "        self.count_links = len(book_hrefs)        \n",
    "        self.book_links = [None]*range(self.count_links)\n",
    "        for link in range(links):\n",
    "            self.book_links[i] = 'https://en.wikipedia.org' + link['href']\n",
    " \n",
    "    def getPages(self):\n",
    "        '''\n",
    "        turn the extracted links into bok page objects will all existing attributes and methods\n",
    "        '''\n",
    "        self.book_pages = [None]*range(l)\n",
    "        for i in range(l):\n",
    "              try self.book_pages[i]= book_page(self.book_links[i])\n",
    "                except print(\"There is no book page for this link\")\n",
    "      \n",
    "    \n",
    "    def getBookTables(self):\n",
    "        '''\n",
    "        Fill the panda tables by using methods fo page objects and the book pages\n",
    "        '''   \n",
    "        for i in range(l):\n",
    "            self.output.loc[\"Title\",i]= self.book_pages[i].name\n",
    "            self.output.loc[\"Author\",i]= self.book_pages[i].author\n",
    "            self.output.loc[\"Country\",i]= self.book_pages[i].author\n",
    "\n",
    "        self.output.MultiIndex(levels = ['Author','Country'])\n",
    "        \n",
    "    \n",
    "\n",
    "#now run the methods one by one to reach the solution\n",
    "T=WIKI_downloader(main_link)\n",
    "T.ListLinks\n",
    "T.getPages\n",
    "T.\n",
    "\n",
    "\n",
    "# and shall the code be working properly, i could play with the pandas right now. \n",
    "# but it's not working, so see you next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 34 books match with a page, the oldest one is from 1993 (the Ambassadors) and most the most frequent author is Joyce wiht two books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
